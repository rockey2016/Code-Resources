# 算法介绍

## K-Means

把n个样本根据他们的属性分为k个聚类，每个聚类满足：

==同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小==。

流程：

![1540804417491](D:\Code-Resources\Articles\assets\1540804417491.png)



假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。

(c-f)演示了聚类的两种迭代。

先划分，把每个数据样本划分到最近的中心点那一簇；

划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。

这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。

==求点群中心点的算法==：**Euclidean Distance 公式**

![1540804497952](D:\Code-Resources\Articles\assets\1540804497952.png)

缺点：

**Kmeans算法的缺陷**



- 聚类中心的个数K 需要事先给定，但在实际中这个 K 值的选定是非常难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适

- Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果。（可以使用Kmeans++算法来解决）

## K-Means ++

**k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。**

流程：

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

对于第3步，如何将D(x)反映到点被选择的概率上，一种算法如下：

1. 先从我们的数据库随机挑个随机点当“种子点”
2. 对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
3. 然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

# Mini Batch K-Means

k-means算法的变体

mini-batches是输入数据的子集，在每一轮训练中随机取样。

MiniBatchKMeans比KMeans收敛更快，但是结果精度也会下降。



# 谱聚类（spectral clustering）

主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。

最常用的相似矩阵的生成方式是基于高斯核距离的全连接方式，最常用的切图方式是Ncut。而到最后常用的聚类方法为K-Means。下面以Ncut总结谱聚类算法流程。

　　　　输入：样本集D=(x1,x2,...,xn)，相似矩阵的生成方式, 降维后的维度k1, 聚类方法，聚类后的维度k2

　　　　输出： 簇划分C(c1,c2,...ck2)).　

　　　　1) 根据输入的相似矩阵的生成方式构建样本的相似矩阵S

　　　　2）根据相似矩阵S构建邻接矩阵W，构建度矩阵D

　　　　3）计算出拉普拉斯矩阵L

　　　　4）构建标准化后的拉普拉斯矩阵![1542261881023](D:\Code-Resources\Articles\assets\1542261881023.png)

　　　　5）计算![1542261888307](D:\Code-Resources\Articles\assets\1542261888307.png)最小的k1个特征值所各自对应的特征向量f

　　　　6) 将各自对应的特征向量f组成的矩阵按行标准化，最终组成n×k1维的特征矩阵F

　　　　7）对F中的每一行作为一个k1维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为k2。

　　　　8）得到簇划分C(c1,c2,...ck2).　　　　　　　　　



# 层次聚类

从下而上地把小的cluster合并聚集，自下而上地进行聚类称为凝聚式层次聚类。

从上而下地将大的cluster进行分割，自上而下地进行聚类称为分裂式层次聚类。

## 起步

层次聚类( `Hierarchical Clustering` )是聚类算法的一种，通过计算不同类别的相似度类创建一个有层次的嵌套的树。

![img](D:\Code-Resources\Articles\assets\v2-feecd653d5efb9bbb77f1e5b38779a51_hd.jpg)

## 层次聚类算法介绍

假设有 n 个待聚类的样本，对于层次聚类算法，它的步骤是：

- 步骤一：（初始化）将每个样本都视为一个聚类；
- 步骤二：计算各个聚类之间的相似度；
- 步骤三：寻找最近的两个聚类，将他们归为一类；
- 步骤四：重复步骤二，步骤三；直到所有样本归为K类，迭代终止。



![img](D:\Code-Resources\Articles\assets\v2-3aed2646f89280472646264b8a740242_hd.jpg)



## 聚类之间的相似度

聚类和聚类之间的相似度有什么来衡量呢？既然是空间中的点，可以采用距离的方式来衡量，一般有下面三种：

**Single Linkage**

 `nearest-neighbor` ，就是取两个类中距离最近的两个样本的距离作为这两个集合的距离。这种计算方式容易造成一种叫做 `Chaining` 的效果，两个 cluster 明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster 。

**Complete Linkage**

这个则完全是 `Single Linkage` 的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点。

**Average Linkage** 这种方法就是把两个集合中的点两两的距离全部放在一起求均值，相对也能得到合适一点的结果。有时异常点的存在会影响均值，平常人和富豪平均一下收入会被拉高是吧，因此这种计算方法的一个变种就是取两两距离的中位数。



# 高维空间聚类

## 子空间聚类



# 密度聚类

## 简介：

1.从数据集中选择一个未处理的样本点

2.以1为圆心,做半径为E的圆,由于圆内圈入点的个数为3,满足密度阈值Minpts,因此称点1为核心对象(黑色实心圆点),且将圈内的4个点形成一个簇,其中点1直接密度可达周围的3个灰色实心原点

3.重复步骤2若干次,其中点1直接密度可达核心对象3,且点2密度可达点3

4.当该过程进行到图Ⅳ,4的E邻域内仅有2个点,小阈值MinPts,因此点4为边缘点(非核心对象),记为ⓧ,继续考察其他点.

5.当所有对象都被考察,该过程结束,得到图Ⅷ.椭圆形内有若干核心对象和边缘点,这些点都是密度相连的.

6.为个点归类,如图Ⅸ:点集黑圈相互密度可达,属于类别1:点集黑三角相互密度可达,属于新的一类,记为类别2;点集白圈与类别1样本点密度相连,属于类别3;点集白三角与类别2样本点密度相连,属于类别4;点ⓧ既非核心对象,也不密度相连,为噪声点.

![1541387610136](D:\Code-Resources\Articles\assets\1541387610136.png)

## DBSCAN

DBscan考虑的是密度，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中（密度可达的簇）。

==核心点==：在半径eps内含有超过Minpts数目的点，则该点为核心点。

边界点：在半径eps内含有小于Minpts数目的点但是在核心点的邻居。 核心点1连接边界点2，边界点2又连接核心点2，则核心点1和边界点2密度可达。

噪音点：任何不是核心点或是边界点的点。

密度：在半径eps内点的数目。

**算法的关键要素：**距离的度量有欧几里德距离，切比雪夫距离等，最近邻搜索算法有Kd_tree, ball_tree

Python中可调的参数：eps和m, eps为半径，m为要求的半径内点的个数即密度，m越大聚出的类越多，因为即要求成某个类的密度要比较高，一旦中间比较稀疏的就不算一个类了；eps越大，类的个数越少。

![1541491189280](D:\Code-Resources\Articles\assets\1541491189280.png)



**优点：**相对抗噪音（可发现离群点），可以发现任意形状的样本。

**缺点：**但计算密度单元的计算复杂度大，但计算密度单元的计算复杂度;不能很好反应高维数据，高维数据不好定义密度



# **混合高斯模型**

**混合高斯模型(GMM)属于软聚类（每个样本可以属于多个类，有概率分布）**GMM认为隐含的类别标签z(i)，服从多项分布



# 评价指标

[轮廓系数(silhouette coefficient)](http://en.wikipedia.org/wiki/Silhouette_(clustering)) 结合了凝聚度和分离度，其计算步骤如下：

1. 对于第 i 个对象，计算它到所属簇中所有其他对象的平均距离，记 ai （体现凝聚度）
2. 对于第 i 个对象和不包含该对象的任意簇，计算该对象到给定簇中所有对象的平均距离，记 bi （体现分离度）
3. 第 i 个对象的轮廓系数为 si = (bi-ai)/max(ai, bi)  //回头研究一下 wordpress 的公式插件去

从上面可以看出，轮廓系数取值为[-1, 1]，其值越大越好，且当值为负时，表明 ai<bi，样本被分配到错误的簇中，聚类结果不可接受。对于接近0的结果，则表明聚类结果有重叠的情况。

对应 scikit-learn 方法是 [sklearn.metrics.silhouette_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)。该方法是计算所有样本的平均值，另一个方法 [silhouette_samples](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples) 会返回所有样本的轮廓系数。

方法包括几个参数，最终返回一个 float 的轮廓系数，通常是在全部样本上的。

- X：二维样本，通常为[n_samples, n_features]，当 metric 设置为”precomputed”时，应为[n_samples, n_samples]方阵
- labels：一维矩阵，每个样本所属簇的 label
- metric：预计算”precomputed”，或者为一个可调用的函数计算两个实例之间的距离。如果为 string，则必须是[metrics.pairwise.pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html#sklearn.metrics.pairwise.pairwise_distances) 中 metric 可选的（‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’ 或‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’）——好多啊。
- sample_size：随机取样一部分计算平均值，int 类型
- random_state：当sample_size 为非空时用来生成随机采样。给定一个种子，或者使用 numpy.RandomState
- **kwds：其他可选的 key-value 参数



可以先对样本进行基于其分布的抽样，然后在小样本范围内进行层次聚类，然后用层次聚类得出的K值，应用于整个样本进行Kmeans聚类。

当知道实际分类时：

Homogeneity（均一性）, 

completeness（完整性） 

V-measure（同时考虑均一性和完整性）

不知道实际分类时：

轮廓系数（求出每个样本的Si，再取平均值就是整体的轮廓系数）

## **Calinski-Harabaz Index**

评价：Calinski-Harabasz分数值ss越大则聚类效果越好，类别内部数据的协方差越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz分数会高。

数值越小可以理解为组间协方差很小，组与组之间界限不明显。 
与轮廓系数的对比，优势：快！相差几百倍！毫秒级



# 结果展示

## 高维数据可视化

常用工具：Plotly，Axes3D，

[在可视分析中系统地结合降维投影与聚类方法](http://vis.pku.edu.cn/blog/%E5%9C%A8%E5%8F%AF%E8%A7%86%E5%88%86%E6%9E%90%E4%B8%AD%E7%B3%BB%E7%BB%9F%E5%9C%B0%E7%BB%93%E5%90%88%E9%99%8D%E7%BB%B4%E6%8A%95%E5%BD%B1%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%88towards-a-s/)

聚类是一种经典的无监督机器学习方法，其目的是根据相似性对数据进行分组，以便于数据的概括描述。聚类算法可分为硬聚类（Hard Clustering）与软聚类（Soft Clustering），前者有“非此即彼”的组别划分，而后者则给出数据在各个类别下的概率分布。硬聚类中，又可根据类别的层次架构分为层次聚类（Hierarchical Clustering）与划分聚类（Partitional Clustering）。其中前者包含了不同层次的分组信息，更适合于大规模、多粒度的数据聚类分析。

同样是无监督的机器学习方法，降维投影则服务于高维数据的可视化。投影算法可分为线性投影（Linear Projection）与非线性投影（Non-Linear Projection）两种。其中前者依托于高维空间中的低维线性平面，后者则能够表现复杂流形上的数据分布。

**结合降维与聚类**

聚类结果：

1、选取最重要的三个特征，使用3d展示

2、使用PCA或factor分析进行降维，维度降维2d或3d，再进行绘图



# 建立模型

建模步骤：

1.变量预处理：缺失值、异常值（用p1或P99替代）、分类变量变成数值型、分类变量转为哑变量、分类变量过多  这里涉及一系列recoding的过程。

2.变量标准化：变量的量纲不一样会引起计算距离的差距。

3.变量的筛选:商业意义、多个维度、变量间相关性。

4.确定分类个数：区分性足够好，又不能太细



# 应用

传统的聚类已经比较成功的解决了低维数据的聚类问题。但是由于实际应用中数据的复杂性，在处理许多问题时，现有的算法经常失效，特别是对于[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)数据和大型数据的情况。因为传统聚类方法在[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)数据集中进行聚类时，主要遇到两个问题。①[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)数据集中存在大量无关的属性使得在所有维中存在簇的可能性几乎为零；②[高维空间](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4)中数据较低维空间中数据分布要稀疏，其中数据间距离几乎相等是普遍现象，而传统聚类方法是基于距离进行聚类的，因此在高维空间中无法基于距离来构建簇。

[高维聚类分析](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90)已成为聚类分析的一个重要研究方向。同时[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)[数据聚类](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB)也是聚类技术的难点。随着技术的进步使得数据收集变得越来越容易，导致数据库规模越来越大、复杂性越来越高，如各种类型的贸易交易数据、Web 文档、基因表达数据等，它们的维度（[属性](https://baike.baidu.com/item/%E5%B1%9E%E6%80%A7)）通常可以达到成百上千维，甚至更高。但是，受“维度效应”的影响，许多在低维数据空间表现良好的聚类方法运用在[高维空间](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4)上往往无法获得好的聚类效果。[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)数据聚类分析是聚类分析中一个非常活跃的领域，同时它也是一个具有挑战性的工作。[高维](https://baike.baidu.com/item/%E9%AB%98%E7%BB%B4)数据聚类分析在市场分析、信息安全、金融、娱乐、反恐等方面都有很广泛的应用。



# 高维空间处理方法

## 主成分分析(PCA Principal Component Analysis)

p代表特征维数，当p=50时，需要p(p-1)/2个二维散列图来描述变量之间的关系。







# 参考

[kmeans实现]:https://www.cnblogs.com/ahu-lichang/p/7161613.html

https://blog.csdn.net/yunqiinsight/article/details/80134331

https://blog.csdn.net/liulingyuan6/article/details/53637812



http://scikit-learn.org/stable/auto_examples/index.html#clustering



【聚类算法】https://www.cnblogs.com/fionacai/p/5873975.html

[谱聚类]http://www.cnblogs.com/pinard/p/6221564.html



[PCA]https://www.cnblogs.com/mikewolf2002/p/3429711.html