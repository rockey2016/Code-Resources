# 概要

文件分类，语义理解

wordnet

深度学习word2vec





# 重要术语

## 词向量

参考：<https://www.cnblogs.com/chenbjin/p/6900339.html>

词向量是计算机将自然语言符号化的重要手段，通过把词或短语映射成低维的实数向量，以向量间的距离来衡量词语的相似性，可作为词语特征进行各项任务，在机器学习算法和自然语言处理中有着广泛应用。



## 准确率和召回率

- Accuracy：表示预测结果的精确度，预测正确的样本数除以总样本数。
- precision: 准确率，又称为查准率，表示预测结果中，预测为正样本的样本中，正确预测为正样本的概率；
- recall: 召回率，又称为查全率，表示在原始样本的正样本中，最后被正确预测为正样本的概率；

![precisionrecallfunciton](/home/star/Resources/Articles/assets/precisionrecallfunciton.png)

![precisionrecall](/home/star/Resources/Articles/assets/precisionrecall.png)

示例：

举个栗子：全班一共30名男生、20名女生。需要机器识别出男生的数量。本次机器一共识别出20名目标对象，其中18名为男性，2名为女性。则
准确率=18/（18+2）=0.9
召回率=18/30=0.6



##　分类模型指标

在二分类问题中，分类器将一个实例的分类标记为是或否，这可以用一个混淆矩阵来表示。混淆矩阵有四个分类:

|                    | actual positive | actual negative |
| ------------------ | --------------- | --------------- |
| predicted positive | TP              | FP              |
| predicted negative | FN              | TN              |



其中，列对应于实例实际所属的类别，行表示分类预测的类别。

- TP（True Positive）：指正确分类的正样本数，即预测为正样本，实际也是正样本。
- FP（False Positive）：指被错误的标记为正样本的负样本数，即实际为负样本而被预测为正样本，所以是False。
- TN（True Negative）：指正确分类的负样本数，即预测为负样本，实际也是负样本。
- FN（False Negative）：指被错误的标记为负样本的正样本数，即实际为正样本而被预测为负样本，所以是False。
- TP+FP+TN+FN：样本总数。
- TP+FN：实际正样本数。
- TP+FP：预测结果为正样本的总数，包括预测正确的和错误的。
- FP+TN：实际负样本数。
- TN+FN：预测结果为负样本的总数，包括预测正确的和错误的。



衍生出多个分类器评价指标:

![img](http://www.fullstackdevel.com/wp-content/uploads/2015/09/53451443447279.png)



在ROC曲线中，以FPR为x轴，TPR为y轴。

FPR指实际负样本中被错误预测为正样本的概率。

TPR指实际正样本中被预测正确的概率。

如下图：

![img](http://www.fullstackdevel.com/wp-content/uploads/2015/09/77541443447279.png)

AUC计算

ROC曲线围住的面积，越大，分类器效果越好。
AUC（area under the curve）就是ROC曲线下方的面积，取值在0.5到1之间，因为随机猜测得到额AUC就是0.5。面积如下图所示，阴影部分即为AUC面积：



在PR曲线中，以Recall为x轴，Precision为y轴。Recall与TPR的意思相同，而Precision指正确分类的正样本数占总正样本的比例。如下图：

![img](http://www.fullstackdevel.com/wp-content/uploads/2015/09/38381443447279.png)



TPR表示当前分到正样本中真实的正样本所占所有正样本的比例

FPR表示当前被错误分到正样本类别中真实的负样本所占所有负样本总数的比例；

**Recall = TPR**，即当前被分到正样本类别中，真实的正样本占所有正样本的比例，即召回率（召回了多少正样本比例）；
**Precision**就是当前划分到正样本类别中，被正确分类的比例（即正式正样本所占比例），就是我们一般理解意义上所关心的正样本的分类准确率；



虽然**Precision** 和 **Recall** 的值我们预期是越高越好，但是这两个值在某些场景下却是存在互斥的。所以在这个意义上，该两处值需要有一定的约束变量来控制。



**ROC曲线和PR曲线的关系**

在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。

ROC和PR曲线都被用于评估机器学习算法对一个给定数据集的分类性能，每个数据集都包含固定数目的正样本和负样本。而ROC曲线和PR曲线之间有着很深的关系。

  **定理1**：对于一个给定的包含正负样本的数据集，ROC空间和PR空间存在一一对应的关系，也就是说，如果recall不等于0，二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。

**定理2**：对于一个给定数目的正负样本数据集，一条曲线在ROC空间中比另一条曲线有优势，当且仅当第一条曲线在PR空间中也比第二条曲线有优势。（这里的“一条曲线比其他曲线有优势”是指其他曲线的所有部分与这条曲线重合或在这条曲线之下。）。



P-R曲线：选取不同阈值时对应的精度和召回画出来

![P_R_curve](/home/star/Resources/Articles/assets/P_R_curve.png)

P-R图直观地显示出学习器在样本总体上的查全率、查准率．总体趋势，精度越高，召回越低，进行比较。

- 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者，如图中学习器A的性能优于学习器C；

- 如果两个学习器的P-R曲线发生了交叉,如图中的A与B，则难以一般性地断言两者孰优孰劣？ 只能在具体的查准率或查全率条件下进行比较．

   在很多情形下，人们往往仍希望把学习器A与B比出个高低．这时一个比较合理的判据是比较P-R曲线下面积的大小， 它在一定程度上表征了学习器在查准率和查全率上取得相对“双高”的比例但这个值不太容易估算,因此，人们设计了一些综合考虑查准率、查全率的性能度量

  “平衡点”（Break Event Point，简称BEP ）就是这样一个度量，它是“查准率＝查全率”时的取值， 如图中学习器C的BEP 是0.64， 而基于BEP的比较，可认为学习器A 优于B.

在实际中常常需要根据具体情况做出取舍，例如一般的搜索情况，在保证召回率的条件下，尽量提升精确率。而像癌症检测、地震检测、金融欺诈等，则在保证精确率的条件下，尽量提升召回率。

引出了一个新的指标F-score,综合考虑Precision和Recall的调和值。

![F-SCORE](/home/star/Resources/Articles/assets/F-SCORE.png)



- 当`β=1`时，称为 `F1-score`或者 `F1-Measure`，这时，精确率和召回率都很重要，权重相同。
- 当有些情况下，我们认为精确率更重要些，那就调整`β`的值小于1，
- 如果我们认为召回率更重要些，那就调整`β`的值大于1。



举个例子：癌症检查数据样本有10000个，其中10个数据祥本是有癌症，其它是无癌症。假设分类模型在无癌症数据9990中预测正确了9980个，在10个癌症数据中预测正确了9个，此时真阳=9，真阴=9980，假阳=10，假阴=1。

```
Accuracy = (9+9980) /10000=99.89% 
Precision=9/(9+10)= 47.36% 
F1-score=2×(47.36% × 90%)/(1×47.36%+90%)=62.07% 
F2-score=5× (47.36% × 90%)/(4×47.36%+90%)=76. 27%
```

# 测试

## MITIE_JIEBA

```
conda activate rasa
cd ~/GIT-PROJECTS/BOT/MITIE/

```

- 构建wordrep工具

```
cd tools/wordrep/
 mkdir build
cd build
cmake ..
cmake --build . --config Release

```



# 文本处理主要流程

## 词性标注

### POS TAG

术语：Part-Of-Speech，词类标签

