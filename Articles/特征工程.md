



# 机器学习系统流程

![1543560274599](D:\Code-Resources\Articles\assets\1543560274599.png)

                                                     ##### 一个标准的机器学习系统流程图

任何智能系统基本上是由一个端到端的流程组成，从数据原始数据开始，利用数据处理技术来加工、处理并从这些数据中设计出有意义的特征和属性。

然后我们通常利用统计模型或机器学习模型在这些特征上建模，如果未来要使用的话，就基于眼前要解决的问题部署模型。



直接输入原始数据并在这些数据基础上直接建模很可能是鲁莽的，因为我们很可能不会得到期望的结果或性能，且算法不够智能，不能自动地从原始数据中抽取有意义的特征（虽然有一些某种程度上自动抽取特征的技术，比如深度学习技术）。



# 特征工程

## 动机

特征工程是构建任何智能系统的必要部分。

自动学习方法：

深度学习

元启发式方法

缺点：每个问题都是针对特定领域的，且更好的特征（适合问题的）通常是系统性能的决定性因素。

备注：

1. 特征工程是一门科学，数据科学家在建模前通常花 70% 的时间用于准备数据，其过程既需要领域知识又需要数学计算。
2. 特征处理是困难的、耗时的且需要专家知识。

## 定义

特征工程是将原始数据转化特征的过程，特征要能更好地表示潜在问题并提高预测模型在未知数据上的准确率。



特征工程是将数据转换为特征的过程，特征是机器学习模型的输入，从而更高质量的特征有助于提高整体模型的性能。

特征的好坏非常地取决于潜在的问题。因此，即使机器学习任务在不同场景中是相同的，比如将邮件分为垃圾邮件或非垃圾邮件，或对手写数字字符进行分类，这两个场景中提取的特征千差万别。

## 主要内容



![img](D:\Code-Resources\Articles\assets\5675957-aff1c96e22f4ca12.webp)

## 特征

一个特征通常是来自原始数据的一种特定表示，它是一个单独的、可度量的属性，通常由数据集中的一列来描述。一个通用的二维数据集，每个样本的观测值用一行来表示，每种特征用一列来表示，从而每个样本的观测值中的各种特征都有一个具体的值。

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f5484b334a.jpg)

每行通常代表一个特征向量，整个特征集包括了所有的观察值形成了二维的特征矩阵，称为特征集。

机器学习算法通常都是处理这些数值型矩阵或张量，因此大部分特征工程技术都将原始数据转换为一些数值型数来表示，使得它们能更好地被算法理解。

### 特征分类

原始特征是直接从数据集中得到，没有额外的操作或处理。

**导出特征**通常来自于特征工程，即我们从现有数据属性中提取的特征。

例如：从一个包含出生日期的雇员数据集中创建一个新的「年龄」特征，只需要将当前日期减去出生日期即可。

根据的类型和格式各不相同，包括结构化的和非结构化的数据。



### 数值型数据上的特征工程

#### 定义：

1. 数值型数据通常以标量的形式表示数据，描述观测值、记录或者测量值。
2. 数值型数据是指连续型数据而不是离散型数据，表示不同类目的数据就是后者。
3. 数值型数据也可以用向量来表示，向量的每个值或分量代表一个特征。
4. 整数和浮点数是连续型数值数据中最常见也是最常使用的数值型数据类型。
5. 即使数值型数据可以直接输入到机器学习模型中，你仍需要在建模前设计与场景、问题和领域相关的特征。

#### 相关工具库

import pandas as pd

import matplotlib.pyplot as plt

import numpy as np

import scipy.stats as spstats

%matplotlib inline



#### 原始度量

原始的度量方法通常用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。

通常这些特征可以表示一些值或总数。

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f55514768e.jpg)



#### 数值

挑选重点特征：仔细地观察原始数据，选取几个代表数值型原始值的属性，可以被直接使用。

目的：对特征中的统计量如总数、平均值、标准差和四分位数有了一个很好的印象。

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f559f61c14.jpg)



#### 计数

原始度量的另一种形式包括代表频率、总数或特征属性发生次数的特征。让我们看看 [**millionsong**](https://labrosa.ee.columbia.edu/millionsong/) **数据集**中的一个例子，其描述了某一歌曲被各种用户收听的总数或频数。

```python
popsong_df = pd.read_csv('datasets/song_views.csv',encoding='utf-8')
popsong_df.head(10)
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f55bf6176f.jpg)

listen_count 字段可以直接作为基于数值型特征的频数或总数。



#### 二值化：

应用：基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。

例如：要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。在这个例子中，二值化的特征比基于计数的特征更合适。我们二值化 listen_count 字段如下：

```
watched = np.array(popsong_df['listen_count'])
watched[watched >= 1] = 1
popsong_df['watched'] = watched
```

使用sklearn的Binarizer

```
from sklearn.preprocessing import Binarizer
bn = Binarizer(threshold=0.9)
pd_watched =bn.transform([popsong_df['listen_count']])[0]
popsong_df['pd_watched'] = pd_watched
popsong_df.head(11)
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f56505e8ff.jpg)



#### 数据舍入：

应用：处理连续型数值属性如比例或百分比时，我们通常不需要高精度的原始数值。因此通常有必要将这些高精度的百分比舍入为整数型数值。这些整数可以直接作为原始数值甚至分类型特征（基于离散类的）使用。

让我们试着将这个观念应用到一个虚拟数据集上，该数据集描述了库存项和他们的流行度百分比。

```
items_popularity =pd.read_csv('datasets/item_popularity.csv',encoding='utf-8')

items_popularity['popularity_scale_10'] = np.array(np.round((items_popularity['pop_percent'] * 10)),dtype='int')

items_popularity['popularity_scale_100'] = np.array(np.round((items_popularity['pop_percent'] * 100)),dtype='int')

items_popularity
```

两种不同的舍入方式。这些特征表明项目流行度的特征现在既有 1-10 的尺度也有 1-100 的尺度。基于这个场景或问题你可以使用这些值同时作为数值型或分类型特征。

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f566e30ad2.jpg)

#### 相关性

高级机器学习模型通常会对作为输入特征变量函数的输出响应建模（离散类别或连续数值）。

线性回归方程可以表示为：

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f56ab22fb7.jpg)

其中输入特征用变量表示为

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f56c69ac66.jpg)

权重或系数可以分别表示为

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f56de74ee7.jpg)

目标是预测响应 **\*y***

仅仅根据单个的、分离的输入特征，这个简单的线性模型描述了输出与输入之间的关系。

在一些真实场景中，有必要试着捕获这些输入特征集一部分的特征变量之间的相关性。上述带有相关特征的线性回归方程的展开式可以简单表示为：

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f5701419ee.jpg)

特征可表示为：

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f57162d4f7.jpg)

表示了相关特征。

```
atk_def = poke_df[['Attack', 'Defense']]

atk_def.head()
```



![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f572bad2cc.jpg)

有两个数值型（连续的）特征，Attack 和 Defence。现在我们可以利用 scikit-learn 建立二度特征。

```
pf = PolynomialFeatures(degree=2,
interaction_only=False,include_bias=False)
res = pf.fit_transform(atk_def)

res

Output
------
array([[ 49., 49., 2401., 2401., 2401.],

        [ 62., 63., 3844., 3906., 3969.],

        [ 82., 83., 6724., 6806., 6889.],

        ...,

        [ 110., 60., 12100., 6600., 3600.],

        [ 160., 60., 25600., 9600., 3600.],

    [ 110., 120., 12100., 13200., 14400.]])
```

上面的特征矩阵一共描述了 5 个特征，其中包括新的相关特征。我们可以看到上述矩阵中每个特征的度，

pd.DataFrame(pf.powers_, columns=['Attack_degree','Defense_degree'])

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f575a65683.jpg)



基于这个输出，现在我们可以通过每个特征的度知道它实际上代表什么。在此基础上，现在我们可以对每个特征进行命名如下。这仅仅是为了便于理解，你可以给这些特征取更好的、容易使用和简单的名字。

```
intr_features = pd.DataFrame(res, columns=['Attack','Defense','Attack^2','Attack x Defense','Defense^2'])

intr_features.head(5)
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f576e91376.jpg)

上述数据代表了我们原始的特征以及它们的相关特征。



## 分区间处理

应用场景：

1. 处理原始、连续的数值型特征问题通常会导致这些特征值的分布被破坏。这表明有些值经常出现而另一些值出现非常少。
2. 另一个问题是这些特征的值的变化范围。比如某个音乐视频的观看总数会非常大，而一些值会非常小。直接使用这些特征会产生很多问题，反而会影响模型表现。
3. 处理方法与技巧：处理这些问题的技巧，包括分区间法和变换。



定义：

分区间（Bining），也叫做量化，用于将连续型数值特征转换为离散型特征（类别）。可以认为这些离散值或数字是类别或原始的连续型数值被分区间或分组之后的数目。每个不同的区间大小代表某种密度，因此一个特定范围的连续型数值会落在里面。对数据做分区间的具体技巧包括等宽分区间以及自适应分区间。

例如：从 [2016 年 FreeCodeCamp 开发者和编码员调查报告](https://github.com/freeCodeCamp/2016-new-coder-survey)中抽取出来的一个子集中的数据，来讨论各种针对编码员和软件开发者的属性。

```
fcc_survey_df =pd.read_csv('datasets/fcc_2016_coder_survey_subset.csv',encoding='utf-8')

fcc_survey_df[['ID.x', 'EmploymentField', 'Age','Income']].head()
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f578e01139.jpg)



### 等宽分区间

1. 在等宽分区间方法中，每个区间都是固定宽度的，通常可以预先分析数据进行定义。

2. 基于一些领域知识、规则或约束，每个区间有个预先固定的值的范围，只有处于范围内的数值才被分配到该区间。

3. 基于数据舍入操作的分区间是一种方式，你可以使用数据舍入操作来对原始值进行分区间

   缺点：

   <u>一些区间中的数据可能会非常的密集，一些区间会非常稀疏甚至是空的</u>

例如：

我们分析编码员调查报告数据集的 Age 特征并看看它的分布：

```
fig, ax = plt.subplots()

fcc_survey_df['Age'].hist(color='#A9C5D3',edgecolor='black',grid=False)

ax.set_title('Developer Age Histogram', fontsize=12)
ax.set_xlabel('Age', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
```



![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f57b05846b.jpg)

如预期那样，开发者年龄分布仿佛往左侧倾斜（上年纪的开发者偏少）。现在我们根据下面的模式，将这些原始年龄值分配到特定的区间。

Age Range: Bin

\---------------

0 - 9 : 0

10 - 19 : 1

20 - 29 : 2

30 - 39 : 3

40 - 49 : 4

50 - 59 : 5

60 - 69 : 6

... and so on

可以简单地使用我们先前学习到的数据舍入部分知识，先将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。

```
fcc_survey_df['Age_bin_round'] = np.array(np.floor(np.array(fcc_survey_df['Age']) / 10.))

fcc_survey_df[['ID.x', 'Age','Age_bin_round']].iloc[1071:1076]
```



![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f57d916a6f.jpg)

扩展：

你可以看到基于数据舍入操作的每个年龄对应的区间。但是如果我们需要更灵活的操作怎么办？如果我们想基于我们的规则或逻辑，确定或修改区间的宽度怎么办？基于常用范围的分区间方法将帮助我们完成这个。让我们来定义一些通用年龄段位，使用下面的方式来对开发者年龄分区间。

Age Range : Bin

\---------------

0 - 15 : 1

16 - 30 : 2

31 - 45 : 3

46 - 60 : 4

61 - 75 : 5

75 - 100 : 6

基于这些常用的分区间方式，我们现在可以对每个开发者年龄值的区间打标签，我们将存储区间的范围和相应的标签。

```
bin_ranges = [0, 15, 30, 45, 60, 75, 100]

bin_names = [1, 2, 3, 4, 5, 6]

fcc_survey_df['Age_bin_custom_range'] = pd.cut(np.array(fcc_survey_df['Age']),bins=bin_ranges)

fcc_survey_df['Age_bin_custom_label'] = pd.cut(np.array(fcc_survey_df['Age']),bins=bin_ranges, labels=bin_names)

# view the binned features

fcc_survey_df[['ID.x', 'Age', 'Age_bin_round','Age_bin_custom_range','Age_bin_custom_label']].iloc[10a71:1076]
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f58143c35f.jpg)

### 自适应分区间

1. 使用等宽分区间的不足之处在于，我们手动决定了区间的值范围，而由于落在某个区间中的数据点或值的数目是不均匀的，因此可能会得到不规则的区间。
2. 自适应分区间方法是一个更安全的策略，在这些场景中，使用数据分布来决定区间的范围。

#### 基于分位数的分区间方法

1、量化对于特定值或切点有助于将特定数值域的连续值分布划分为离散的互相挨着的区间。因此 q 分位数有助于将数值属性划分为 q 个相等的部分。

2、==2 分位数（中值）==：将数据分布划分为2个相等的区间；==4 分位数（简称分位数）==：它将数据划分为 4 个相等的区间；==10 分位数（十分位数）==：创建 10 个相等宽度的区间

示例：开发者数据集的 Income 字段的数据分布

```
fig, ax = plt.subplots()
fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3',edgecolor='black',grid=False)

ax.set_title('Developer Income Histogram',fontsize=12)
ax.set_xlabel('Developer Income', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
```



![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f583631eff.jpg)

基于自适应分箱方式做一个 4-分位数或分位数:

```
quantile_list = [0, .25, .5, .75, 1.]
quantiles = fcc_survey_df['Income'].quantile(quantile_list)

quantiles

 

Output
------
0.00 6000.0

0.25 20000.0

0.50 37000.0

0.75 60000.0

1.00 200000.0

Name: Income, dtype: float64
```

在原始的分布直方图中可视化下这些分位数:

```
fig, ax = plt.subplots()

fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3',edgecolor='black',grid=False)

for quantile in quantiles:

    qvl = plt.axvline(quantile, color='r')

ax.legend([qvl], ['Quantiles'], fontsize=10)

ax.set_title('Developer Income Histogram with Quantiles',fontsize=12)

ax.set_xlabel('Developer Income', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)
```

上面描述的分布中红色线代表了分位数值和我们潜在的区间。让我们利用这些知识来构建我们基于分区间策略的分位数:

```
quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']

fcc_survey_df['Income_quantile_range'] = pd.qcut(

fcc_survey_df['Income'],q=quantile_list)

fcc_survey_df['Income_quantile_label'] = pd.qcut(

fcc_survey_df['Income'],q=quantile_list,labels=quantile_labels)

fcc_survey_df[['ID.x', 'Age', 'Income','Income_quantile_range',

'Income_quantile_label']].iloc[4:9]
```



![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f586dbd8f4.jpg)

​                                             基于分位数的开发者收入的区间范围和标签

分区间的结果是离散值类型的分类特征，当你在模型中使用分类数据之前，可能需要额外的特征工程相关步骤.







## 统计变换

应用：数据分布倾斜的负面影响。

这是一个特征工程技巧，即利用统计或数学变换，有Log 变换和 Box-Cox 变换。

结果：这两种变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。

### Log变换

log 变换属于幂变换函数簇。该函数用数学表达式表示为

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f588a0f6a5.jpg)

表示以b为底指数必须达到多少才等于x。

自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。

也可以使用通常在十进制系统中使用的 b=10 作为底数。

**当应用于倾斜分布时 Log 变换是很有用的，因为他们倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围**。从而使得倾斜分布尽可能的接近正态分布。

例如：对先前使用的开发者数据集的 Income 特征上使用log变换。

```
fcc_survey_df['Income_log'] = np.log((1+fcc_survey_df['Income']))

fcc_survey_df[['ID.x', 'Age', 'Income','Income_log']].iloc[4:9]
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f58b3ed249.jpg)

Income_log 字段描述了经过 log 变换后的特征。现在让我们来看看字段变换后数据的分布。

基于上面的图，我们可以清楚地看到与先前倾斜分布相比，该分布更加像正态分布或高斯分布。

```
income_log_mean =np.round(np.mean(fcc_survey_df['Income_log']), 2)

fig, ax = plt.subplots()

fcc_survey_df['Income_log'].hist(bins=30,color='#A9C5D3',edgecolor='black',grid=False)

plt.axvline(income_log_mean, color='r')
ax.set_title('Developer Income Histogram after Log Transform',fontsize=12)
ax.set_xlabel('Developer Income (log scale)',fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.text(11.5, 450, r'$\mu$='+str(income_log_mean),fontsize=10)
```

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f58cdaf02a.jpg)

### Box-Cox变换

Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。数学上，Box-Cox 变换函数可以表示如下。

![ä¸ä¼åç¹å¾å·¥ç¨ç AI ç ç©¶åä¸æ¯å¥½æ°æ®ç§å­¦å®¶ï¼ä¸ç¯ - è¿ç"­æ°æ®çå¤çæ¹æ³](D:\Code-Resources\Articles\assets\5a5f58e556c08.jpg)

生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。现在让我们在开发者数据集的收入特征上应用 Box-Cox 变换。首先我们从数据分布中移除非零值得到最佳的值，结果如下:

```
income = np.array(fcc_survey_df['Income'])
income_clean = income[~np.isnan(income)]
l, opt_lambda = spstats.boxcox(income_clean)
print('Optimal lambda value:', opt_lambda)
```



# 数据集特性

## 不平衡数据

实例场景：

场景1：

“在我的训练样本里有一个二项分类问题，其中有一个数据集是比例为60:1的数据集，于是我对它运用了逻辑回归的训练方法，但是得到的结果是忽略这一个比率为60:1的数据集的训练结果。

场景2：

“现在我正在运行一个分类模型。在我的数据集里面一共有3类数据，这里我们称它们分别为A，B和C，但是在我的训练数据集里面A，B和C三类数据分别占了70%，25%和5%。在大多数情况下，结果都过度拟合A类数据。你能给我一些建议来解决这个问题吗？

解决方法：

1、可以扩大数据样本

扩大样本数据总是容易被忽视。

一个更大的数据集，就有可能挖掘出不同的或许更平衡的方面。  

当我们寻找重复采集的数据样本时，一些小样本类数据的例子可能是有帮助的。

2、改变绩效标准

精度是一个不适用于不平衡的数据集的绩效指标。

在处理不平衡类时，有些更加理想的指标可以给你更加具有说服力的结果。

相比于传统的精确度，这些绩效标准可以更加深入地洞察模型的准确率：

- 混淆矩阵：将要预测的数据分到表里来显示正确的预测（对角线），并了解其不正确的预测的类型（哪些类被分配了不正确的预测）；
- 精度：一种分类准确性的处理方法；
- 召回率：一种分类完整性的处理方法；
- F1分数（或F-分）：精度和召回率的加权平均。
- Kappa（或者[Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen's_kappa)）：根据数据中集合数据的不平衡点来标准化分类精度；
- ROC曲线：类似于精度和召回率，准确性被分为敏感性和特异性，并且可以基于这些值的平衡阈值来选择模型。



3、尝试对你的数据重新抽样

- 可以改变将要用来建立预测模型的数据集来获得更加平衡的数据集，这种变化被称为抽样数据集。
- 方法：

​      从代表性不足的类（又称为过抽样或者更加正式的抽样来代替）添加实例的副本；

​     从过度代表类里删除实例，称为抽样不足。

   ==google参考Oversampling and undersampling in data analysis==





过程：

- 特征的理解
- 识别噪声特征(最有趣的部分!)
- 特征工程
- 重要特征选择
- 特征的调试
- 遗漏检测和理解
- 模型监控

https://www.leiphone.com/news/201811/YbdYAzttj1rDIOVg.html



# 相关性分析

## 相关性

相关系数的取值范围为[-1,1]，

当相关系数为1时。表示正相关；

当相关系数为-1时，表示负相关；

当相关系数为0时，表示不相关。 
正相关：因变量随着自变量的增大而增大 
负相关：因变量随着自变量的增大而减小

计算公式：

![1545886128857](D:\Code-Resources\Articles\assets\1545886128857.png)

## 相关矩阵

相关矩阵中每一个值都是代表原矩阵中各列之间的相关系数（相关矩阵为方阵，阶数为原矩阵的列数），对角线上都是原矩阵各列与自身的相关系数，所以对角线的值均为1。

**典型关联分析(Canonical Correlation Analysis，简称CCA)是最常用的挖掘数据关联关系的算法之一**。

**CCA算法广泛的应用于数据相关度的分析，同时还是偏最小二乘法的基础。**但是由于它依赖于数据的线性表示，当我们的数据无法线性表示时，CCA就无法使用，此时我们可以利用核函数的思想，将数据映射到高维后，再利用CCA的思想降维到1维，求对应的相关系数和线性关系，这个算法一般称为KCCA。

# 通用工具

## t-SNE

### 概述

（t-SNE）t-分布式随机邻域嵌入是一种用于挖掘高维数据的非线性降维算法。 

它将多维数据映射到适合于人类观察的两个或多个维度。 在t-SNE算法的帮助下，**你下一次使用高维数据时，可能就不需要绘制很多探索性数据分析图了。**

t-SNE通过基于具有多个特征的数据点的相似性识别观察到的模式来找到数据中的规律。

t-SNE不是一个聚类算法，而是一个降维算法。因为当它把高维数据映射到低维空间时，原数据中的特征值不复存在。所以不能仅基于t-SNE的输出进行任何推断。

本质上它主要是一种数据探索和可视化技术。

但是t-SNE可以用于分类器和聚类中，用它来生成其他分类算法的输入特征值。



==降维算法==

1.主成分分析（线性）

2.t-SNE（非参数/非线性）

3.萨蒙映射（非线性）

4.等距映射（非线性）

5.局部线性嵌入(非线性)

6.规范相关分析（非线性）

7.SNE(非线性)

8.最小方差无偏估计（非线性）

9.拉普拉斯特征图（非线性）



### 应用场景

**数据科学家**

对于数据科学家来说，使用t-SNE的主要问题是算法的黑盒类型性质。这阻碍了基于结果提供推论和洞察的过程。此外，该算法的另一个问题是它不一定在连续运行时永远产生类似的输出。

那么，你怎么能使用这个算法？最好的使用方法是用它进行探索性数据分析。 它会给你非常明确地展示数据内隐藏的模式。它也可以用作其他分类和聚类算法的输入参数。

**机器学习竞赛爱好者**

将数据集减少到2或3个维度，并使用非线性堆栈器将其堆栈。 使用保留集进行堆叠/混合。 然后你可以使用XGboost提高t-SNE向量以得到更好的结果。

**数据科学爱好者**

对于才开始接触数据科学的数据科学爱好者来说，这种算法在研究和性能增强方面提供了最好的机会。已经有一些研究论文尝试通过利用线性函数来提高算法的时间复杂度。但是尚未得到理想的解决方案。针对各种实施t-SNE算法解决自然语言处理问题和图像处理应用程序的研究论文是一个尚未开发的领域，并且有足够的空间范围。

### 常见错误

以下是在解释t-SNE的结果时要注意的几个点：

1.为了使算法正确执行，困惑度应小于数据点数。 此外，推荐的困惑度在（5至50）范围内

2.有时，具有相同超参数的多次运行结果可能彼此不同。

3.任何t-SNE图中的簇大小不得用于标准偏差，色散或任何其他诸如此类的度量。这是因为t-SNE扩展更密集的集群，并且使分散的集群收缩到均匀的集群大小。 这是它产生清晰的地块的原因之一。

4.簇之间的距离可以改变，因为全局几何与最佳困惑度密切相关。 在具有许多元素数量不等的簇的数据集中，同一个困惑度不能优化所有簇的距离。

5.模式也可以在随机噪声中找到，因此在决定数据中是否存在模式之前，必须检查具有不同的超参数组的多次运算结果。

6.在不同的困惑度水平可以观察到不同的簇形状。

7.拓扑不能基于单个t-SNE图来分析，在进行任何评估之前必须观察多个图。



https://distill.pub/2016/misread-tsne/

https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne



==PCA==

PCA是一种线性算法。 它不能解释特征之间的复杂多项式关系。 

相比较，t-SNE是基于在邻域图上随机游走的概率分布，可以在数据中找到其结构关系。

线性降维算法的一个主要问题是它们集中将不相似的数据点放置在较低维度区域时，数据点相距甚远。 但是为了在低维、非线性流型上表示高维数据，我们也需要把相似的数据点靠近在一起展示，这并不是线性降维算法所能做的。

https://www.plob.org/article/11918.html





参考：

https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html

https://github.com/freeCodeCamp/2016-new-coder-survey

https://www.leiphone.com/news/201801/jPN08If0nKYfI2Xs.html



























