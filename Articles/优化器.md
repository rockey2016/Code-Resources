# Adam

\# 从 keras.optimizers 导入 Adam 模块

from keras.optimizers import Adam

## 算法

![1531451640335](D:\Manshy\DFC_ML\health_predict\docs\adam-algorithm.png)



class Adam(Optimizer):

​    """

​    # 参数

​        lr: float >= 0. 学习速率、学习步长，值越大则表示权值调整动作越大，对应上图算法中的参数 alpha；

​        beta_1:  接近 1 的常数，（有偏）一阶矩估计的指数衰减因子；

​        beta_2:  接近 1 的常数，（有偏）二阶矩估计的指数衰减因子；

​        epsilon: 大于但接近 0 的数，放在分母，避免除以 0 ；

​        decay:  学习速率衰减因子，【2】算法中无这个参数；

​    """



# SGD

### Time-Based Learning Rate Schedule

epochs = 50

learning_rate = 0.1

decay_rate = learning_rate / epochs

momentum = 0.8

sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)



# 梯度下降

![1553673177534](D:\Code-Resources\Articles\assets\1553673177534.png)



传统梯度下降：

 又称为  **BGD(batch gradient descent)**

每次迭代计算所有样本的平均梯度，全部数据更新完更新一次权重。

  **batch_size为full batch**，下降一直趋于拟合，但每次更新权重花费较长时间。



随机梯度下降：

   **batch_size为1**，每一次迭代更新权重只需要训练一个数据。

**通过小的学习率，噪声也会小很多，但舍弃了向量化处理带来的加速**。



mini-batch梯度下降：

**每一个batch更新一次去权值。下降会有一些噪声变化，但是总体趋势是走向拟合中心的。**

**每次训练的不能保证使用的是同一份数据，所以每一个batch不能保证都下降，整体训练loss变化会有很多噪声，但是整体趋势是下降的，随后会在最优值附近波动，不会收敛。**



<https://blog.csdn.net/zhao_crystal/article/details/80472720>